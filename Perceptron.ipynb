{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3e9ea302-aeb8-4475-b860-8196f5d2ad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "def analyze_feat(data):\n",
    "    feat_col = [col for col in data.columns if col != 'label']\n",
    "    print(f\"資料集基本資訊:\")\n",
    "    print(f\"訓練集大小: {data.shape}\")\n",
    "    print(f\"特徵數量: {len(feat_col)}\")\n",
    "\n",
    "    # 分析標籤分布\n",
    "    print(\"\\n標籤分布:\")\n",
    "    print(data['label'].value_counts())\n",
    "\n",
    "    # 檢查基本統計資訊\n",
    "    print(\"\\n特徵統計摘要:\")\n",
    "    print(data[feat_col].describe())\n",
    "\n",
    "def remove_zero_var_feat(data):\n",
    "    zero_var_feat = []\n",
    "    feature_columns = [col for col in data.columns if col != 'label']\n",
    "\n",
    "    for feat in feature_columns:\n",
    "        if data[feat].nunique() == 1:\n",
    "            zero_var_feat.append(feat)\n",
    "\n",
    "    data_cleaned = data.drop(columns=zero_var_feat)\n",
    "    return data_cleaned\n",
    "\n",
    "def trim_outliers(data, percentile=0.01):\n",
    "    \"\"\"裁剪極端值而非完全移除\"\"\"\n",
    "    feature_columns = [col for col in data.columns if col != 'label']\n",
    "    data_trimmed = data.copy()\n",
    "    \n",
    "    for feat in feature_columns:\n",
    "        lower = data[feat].quantile(percentile)\n",
    "        upper = data[feat].quantile(1-percentile)\n",
    "        data_trimmed[feat] = data_trimmed[feat].clip(lower, upper)\n",
    "    return data_trimmed\n",
    "\n",
    "def identify_zero_var_feat(data):\n",
    "    zero_var_feat = []\n",
    "    feature_columns = [col for col in data.columns if col != 'label']\n",
    "    for feat in feature_columns:\n",
    "        if data[feat].nunique() == 1:\n",
    "            zero_var_feat.append(feat)\n",
    "    return zero_var_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e7e77429-bd73-40f3-8aa5-f325d7b3b0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3428c722-c17d-40f4-947c-9578ad582670",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.dropna()\n",
    "train_cleaned = remove_zero_var_feat(train)\n",
    "train_clean_and_trim = trim_outliers(train_cleaned)\n",
    "# train_clean_and_trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "56346615-7aac-44d1-bd11-fcdb3936a373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC5</th>\n",
       "      <th>PC6</th>\n",
       "      <th>PC7</th>\n",
       "      <th>PC8</th>\n",
       "      <th>PC9</th>\n",
       "      <th>...</th>\n",
       "      <th>PC68</th>\n",
       "      <th>PC69</th>\n",
       "      <th>PC70</th>\n",
       "      <th>PC71</th>\n",
       "      <th>PC72</th>\n",
       "      <th>PC73</th>\n",
       "      <th>PC74</th>\n",
       "      <th>PC75</th>\n",
       "      <th>PC76</th>\n",
       "      <th>PC77</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.891636</td>\n",
       "      <td>-0.551380</td>\n",
       "      <td>0.763197</td>\n",
       "      <td>-1.705115</td>\n",
       "      <td>0.503612</td>\n",
       "      <td>-1.290867</td>\n",
       "      <td>1.034565</td>\n",
       "      <td>1.952948</td>\n",
       "      <td>0.570574</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091739</td>\n",
       "      <td>0.030523</td>\n",
       "      <td>-0.060181</td>\n",
       "      <td>0.013141</td>\n",
       "      <td>-0.129054</td>\n",
       "      <td>-0.109666</td>\n",
       "      <td>-0.250439</td>\n",
       "      <td>-0.048888</td>\n",
       "      <td>0.013114</td>\n",
       "      <td>-0.004277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>-0.648400</td>\n",
       "      <td>-0.671903</td>\n",
       "      <td>0.319544</td>\n",
       "      <td>-1.168103</td>\n",
       "      <td>-1.157278</td>\n",
       "      <td>-0.564319</td>\n",
       "      <td>2.255324</td>\n",
       "      <td>1.112096</td>\n",
       "      <td>1.649733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056364</td>\n",
       "      <td>-0.002835</td>\n",
       "      <td>0.050115</td>\n",
       "      <td>0.058307</td>\n",
       "      <td>0.223791</td>\n",
       "      <td>0.034248</td>\n",
       "      <td>0.067557</td>\n",
       "      <td>-0.038378</td>\n",
       "      <td>-0.050100</td>\n",
       "      <td>0.035227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>7.779782</td>\n",
       "      <td>-2.775658</td>\n",
       "      <td>-2.984205</td>\n",
       "      <td>2.237394</td>\n",
       "      <td>-4.175191</td>\n",
       "      <td>1.541019</td>\n",
       "      <td>-0.665241</td>\n",
       "      <td>-4.292417</td>\n",
       "      <td>1.920400</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077553</td>\n",
       "      <td>0.177987</td>\n",
       "      <td>-0.020185</td>\n",
       "      <td>0.020134</td>\n",
       "      <td>0.030449</td>\n",
       "      <td>-0.047858</td>\n",
       "      <td>-0.134861</td>\n",
       "      <td>0.160203</td>\n",
       "      <td>0.045374</td>\n",
       "      <td>-0.270493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>-0.993142</td>\n",
       "      <td>0.062101</td>\n",
       "      <td>0.966048</td>\n",
       "      <td>1.765415</td>\n",
       "      <td>1.188964</td>\n",
       "      <td>3.097109</td>\n",
       "      <td>1.183999</td>\n",
       "      <td>-1.496847</td>\n",
       "      <td>-0.016068</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010328</td>\n",
       "      <td>0.047674</td>\n",
       "      <td>0.094035</td>\n",
       "      <td>0.012736</td>\n",
       "      <td>0.005438</td>\n",
       "      <td>0.050830</td>\n",
       "      <td>-0.037208</td>\n",
       "      <td>-0.012051</td>\n",
       "      <td>0.064610</td>\n",
       "      <td>-0.151841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>-2.741324</td>\n",
       "      <td>0.822105</td>\n",
       "      <td>1.608229</td>\n",
       "      <td>0.992427</td>\n",
       "      <td>1.334669</td>\n",
       "      <td>0.597561</td>\n",
       "      <td>2.029869</td>\n",
       "      <td>0.190938</td>\n",
       "      <td>-1.155607</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032742</td>\n",
       "      <td>-0.013228</td>\n",
       "      <td>-0.014584</td>\n",
       "      <td>-0.016860</td>\n",
       "      <td>-0.044201</td>\n",
       "      <td>0.126317</td>\n",
       "      <td>-0.000882</td>\n",
       "      <td>0.021117</td>\n",
       "      <td>-0.003464</td>\n",
       "      <td>-0.024336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7592</th>\n",
       "      <td>-1</td>\n",
       "      <td>-6.025926</td>\n",
       "      <td>-0.178534</td>\n",
       "      <td>0.031776</td>\n",
       "      <td>0.189696</td>\n",
       "      <td>-0.019145</td>\n",
       "      <td>-0.511366</td>\n",
       "      <td>-1.945743</td>\n",
       "      <td>-0.680624</td>\n",
       "      <td>-0.028814</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041036</td>\n",
       "      <td>0.012827</td>\n",
       "      <td>-0.007174</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.011178</td>\n",
       "      <td>-0.023759</td>\n",
       "      <td>-0.005371</td>\n",
       "      <td>-0.003055</td>\n",
       "      <td>-0.005612</td>\n",
       "      <td>0.006916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7593</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.347762</td>\n",
       "      <td>0.064158</td>\n",
       "      <td>-1.556552</td>\n",
       "      <td>-4.105600</td>\n",
       "      <td>-2.892081</td>\n",
       "      <td>-2.566729</td>\n",
       "      <td>-0.874057</td>\n",
       "      <td>-0.439323</td>\n",
       "      <td>-0.083176</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096727</td>\n",
       "      <td>-0.004021</td>\n",
       "      <td>-0.036147</td>\n",
       "      <td>0.043945</td>\n",
       "      <td>0.146055</td>\n",
       "      <td>-0.176540</td>\n",
       "      <td>-0.020394</td>\n",
       "      <td>-0.004482</td>\n",
       "      <td>0.051724</td>\n",
       "      <td>-0.013379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7594</th>\n",
       "      <td>-1</td>\n",
       "      <td>-0.744512</td>\n",
       "      <td>-0.108301</td>\n",
       "      <td>-0.714964</td>\n",
       "      <td>-1.321755</td>\n",
       "      <td>-1.723064</td>\n",
       "      <td>3.041560</td>\n",
       "      <td>1.419419</td>\n",
       "      <td>1.535666</td>\n",
       "      <td>1.925135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194754</td>\n",
       "      <td>0.045216</td>\n",
       "      <td>-0.031984</td>\n",
       "      <td>0.140548</td>\n",
       "      <td>0.268245</td>\n",
       "      <td>0.052348</td>\n",
       "      <td>0.068755</td>\n",
       "      <td>-0.002507</td>\n",
       "      <td>-0.027687</td>\n",
       "      <td>-0.048845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7595</th>\n",
       "      <td>-1</td>\n",
       "      <td>-3.703554</td>\n",
       "      <td>2.590339</td>\n",
       "      <td>4.357971</td>\n",
       "      <td>0.259575</td>\n",
       "      <td>-2.270371</td>\n",
       "      <td>-1.713973</td>\n",
       "      <td>-4.245264</td>\n",
       "      <td>1.054712</td>\n",
       "      <td>0.494570</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021935</td>\n",
       "      <td>0.095566</td>\n",
       "      <td>-0.000888</td>\n",
       "      <td>-0.043632</td>\n",
       "      <td>-0.010393</td>\n",
       "      <td>-0.025836</td>\n",
       "      <td>0.054332</td>\n",
       "      <td>0.006666</td>\n",
       "      <td>0.023839</td>\n",
       "      <td>-0.057499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7596</th>\n",
       "      <td>-1</td>\n",
       "      <td>9.251954</td>\n",
       "      <td>3.275927</td>\n",
       "      <td>3.811275</td>\n",
       "      <td>-3.160661</td>\n",
       "      <td>0.159884</td>\n",
       "      <td>3.549016</td>\n",
       "      <td>0.396316</td>\n",
       "      <td>2.762974</td>\n",
       "      <td>0.893582</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058264</td>\n",
       "      <td>-0.183170</td>\n",
       "      <td>-0.377547</td>\n",
       "      <td>-0.083935</td>\n",
       "      <td>-0.218222</td>\n",
       "      <td>0.200268</td>\n",
       "      <td>0.177420</td>\n",
       "      <td>0.077696</td>\n",
       "      <td>-0.022946</td>\n",
       "      <td>0.156416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7597 rows × 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label       PC1       PC2       PC3       PC4       PC5       PC6  \\\n",
       "0         1  0.891636 -0.551380  0.763197 -1.705115  0.503612 -1.290867   \n",
       "1        -1 -0.648400 -0.671903  0.319544 -1.168103 -1.157278 -0.564319   \n",
       "2        -1  7.779782 -2.775658 -2.984205  2.237394 -4.175191  1.541019   \n",
       "3        -1 -0.993142  0.062101  0.966048  1.765415  1.188964  3.097109   \n",
       "4        -1 -2.741324  0.822105  1.608229  0.992427  1.334669  0.597561   \n",
       "...     ...       ...       ...       ...       ...       ...       ...   \n",
       "7592     -1 -6.025926 -0.178534  0.031776  0.189696 -0.019145 -0.511366   \n",
       "7593     -1 -1.347762  0.064158 -1.556552 -4.105600 -2.892081 -2.566729   \n",
       "7594     -1 -0.744512 -0.108301 -0.714964 -1.321755 -1.723064  3.041560   \n",
       "7595     -1 -3.703554  2.590339  4.357971  0.259575 -2.270371 -1.713973   \n",
       "7596     -1  9.251954  3.275927  3.811275 -3.160661  0.159884  3.549016   \n",
       "\n",
       "           PC7       PC8       PC9  ...      PC68      PC69      PC70  \\\n",
       "0     1.034565  1.952948  0.570574  ...  0.091739  0.030523 -0.060181   \n",
       "1     2.255324  1.112096  1.649733  ...  0.056364 -0.002835  0.050115   \n",
       "2    -0.665241 -4.292417  1.920400  ... -0.077553  0.177987 -0.020185   \n",
       "3     1.183999 -1.496847 -0.016068  ... -0.010328  0.047674  0.094035   \n",
       "4     2.029869  0.190938 -1.155607  ... -0.032742 -0.013228 -0.014584   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "7592 -1.945743 -0.680624 -0.028814  ...  0.041036  0.012827 -0.007174   \n",
       "7593 -0.874057 -0.439323 -0.083176  ...  0.096727 -0.004021 -0.036147   \n",
       "7594  1.419419  1.535666  1.925135  ...  0.194754  0.045216 -0.031984   \n",
       "7595 -4.245264  1.054712  0.494570  ... -0.021935  0.095566 -0.000888   \n",
       "7596  0.396316  2.762974  0.893582  ...  0.058264 -0.183170 -0.377547   \n",
       "\n",
       "          PC71      PC72      PC73      PC74      PC75      PC76      PC77  \n",
       "0     0.013141 -0.129054 -0.109666 -0.250439 -0.048888  0.013114 -0.004277  \n",
       "1     0.058307  0.223791  0.034248  0.067557 -0.038378 -0.050100  0.035227  \n",
       "2     0.020134  0.030449 -0.047858 -0.134861  0.160203  0.045374 -0.270493  \n",
       "3     0.012736  0.005438  0.050830 -0.037208 -0.012051  0.064610 -0.151841  \n",
       "4    -0.016860 -0.044201  0.126317 -0.000882  0.021117 -0.003464 -0.024336  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "7592  0.000309  0.011178 -0.023759 -0.005371 -0.003055 -0.005612  0.006916  \n",
       "7593  0.043945  0.146055 -0.176540 -0.020394 -0.004482  0.051724 -0.013379  \n",
       "7594  0.140548  0.268245  0.052348  0.068755 -0.002507 -0.027687 -0.048845  \n",
       "7595 -0.043632 -0.010393 -0.025836  0.054332  0.006666  0.023839 -0.057499  \n",
       "7596 -0.083935 -0.218222  0.200268  0.177420  0.077696 -0.022946  0.156416  \n",
       "\n",
       "[7597 rows x 78 columns]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 前處理並標準化\n",
    "X = train_clean_and_trim.drop('label', axis=1)\n",
    "Y = train_clean_and_trim[['label']]  # 使用同一個清理好的資料框獲取標籤\n",
    "\n",
    "# 標準化並執行PCA\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "# 獲取解釋方差\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "n_components = np.argmax(cumulative_variance >= 0.999) + 1  # 改回95%或根據需要調整\n",
    "\n",
    "# 應用PCA降維\n",
    "X_pca = pca.transform(X_scaled)[:, :n_components]\n",
    "feature_names = [f'PC{i+1}' for i in range(n_components)]\n",
    "X_pca_df = pd.DataFrame(X_pca, columns=feature_names, index=X.index)\n",
    "\n",
    "# 合併標籤\n",
    "train_pca = pd.concat([Y, X_pca_df], axis=1)\n",
    "train_pca['label'] = train_pca['label'].map({0: -1, 1: 1})\n",
    "\n",
    "train_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "e34d2753-aeb2-4a1c-b09c-f5d7c73d11b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "準確率: 0.8296052631578947\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.91      0.82      0.86       993\n",
      "           1       0.72      0.84      0.77       527\n",
      "\n",
      "    accuracy                           0.83      1520\n",
      "   macro avg       0.81      0.83      0.82      1520\n",
      "weighted avg       0.84      0.83      0.83      1520\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jjhuang/MSD/CS6350/cs6350/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jjhuang/MSD/CS6350/cs6350/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jjhuang/MSD/CS6350/cs6350/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jjhuang/MSD/CS6350/cs6350/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jjhuang/MSD/CS6350/cs6350/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jjhuang/MSD/CS6350/cs6350/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jjhuang/MSD/CS6350/cs6350/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jjhuang/MSD/CS6350/cs6350/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jjhuang/MSD/CS6350/cs6350/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jjhuang/MSD/CS6350/cs6350/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳參數: {'C': 1000000, 'solver': 'liblinear'}\n",
      "最佳分數: 0.8395428086344895\n"
     ]
    }
   ],
   "source": [
    "# Logistic\n",
    "# 分割資料集\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_pca_df, \n",
    "    train_pca['label'], \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 創建並訓練 Logistic Regression 模型\n",
    "lr = LogisticRegression(max_iter=10000, random_state=42)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# 評估模型\n",
    "y_pred = lr.predict(X_val)\n",
    "print(f\"準確率: {accuracy_score(y_val, y_pred)}\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "# 如果要調整超參數\n",
    "param_grid = {\n",
    "    'C': [1000000, 100000],  # 1000000\n",
    "    'solver': ['liblinear', 'saga'] #liblinear\n",
    "}\n",
    "grid = GridSearchCV(LogisticRegression(random_state=42), param_grid, cv=5)\n",
    "grid.fit(X_pca_df, train_pca['label'])\n",
    "print(f\"最佳參數: {grid.best_params_}\")\n",
    "print(f\"最佳分數: {grid.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "c5138e03-2c00-4354-a36c-38c4052811b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "測試資料準確率: 0.8345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5c/g8dnzwpn14sbd87g_ts9nq980000gn/T/ipykernel_3727/2830798469.py:48: FutureWarning: Downcasting behavior in Series and DataFrame methods 'where', 'mask', and 'clip' is deprecated. In a future version this will not infer object dtypes or cast all-round floats to integers. Instead call result.infer_objects(copy=False) for object inference, or cast round floats explicitly. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data_trimmed[feat] = data_trimmed[feat].clip(lower, upper)\n",
      "/var/folders/5c/g8dnzwpn14sbd87g_ts9nq980000gn/T/ipykernel_3727/2830798469.py:48: FutureWarning: Downcasting behavior in Series and DataFrame methods 'where', 'mask', and 'clip' is deprecated. In a future version this will not infer object dtypes or cast all-round floats to integers. Instead call result.infer_objects(copy=False) for object inference, or cast round floats explicitly. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data_trimmed[feat] = data_trimmed[feat].clip(lower, upper)\n",
      "/var/folders/5c/g8dnzwpn14sbd87g_ts9nq980000gn/T/ipykernel_3727/2830798469.py:48: FutureWarning: Downcasting behavior in Series and DataFrame methods 'where', 'mask', and 'clip' is deprecated. In a future version this will not infer object dtypes or cast all-round floats to integers. Instead call result.infer_objects(copy=False) for object inference, or cast round floats explicitly. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data_trimmed[feat] = data_trimmed[feat].clip(lower, upper)\n",
      "/var/folders/5c/g8dnzwpn14sbd87g_ts9nq980000gn/T/ipykernel_3727/2830798469.py:48: FutureWarning: Downcasting behavior in Series and DataFrame methods 'where', 'mask', and 'clip' is deprecated. In a future version this will not infer object dtypes or cast all-round floats to integers. Instead call result.infer_objects(copy=False) for object inference, or cast round floats explicitly. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data_trimmed[feat] = data_trimmed[feat].clip(lower, upper)\n",
      "/var/folders/5c/g8dnzwpn14sbd87g_ts9nq980000gn/T/ipykernel_3727/2830798469.py:48: FutureWarning: Downcasting behavior in Series and DataFrame methods 'where', 'mask', and 'clip' is deprecated. In a future version this will not infer object dtypes or cast all-round floats to integers. Instead call result.infer_objects(copy=False) for object inference, or cast round floats explicitly. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data_trimmed[feat] = data_trimmed[feat].clip(lower, upper)\n",
      "/var/folders/5c/g8dnzwpn14sbd87g_ts9nq980000gn/T/ipykernel_3727/2830798469.py:48: FutureWarning: Downcasting behavior in Series and DataFrame methods 'where', 'mask', and 'clip' is deprecated. In a future version this will not infer object dtypes or cast all-round floats to integers. Instead call result.infer_objects(copy=False) for object inference, or cast round floats explicitly. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data_trimmed[feat] = data_trimmed[feat].clip(lower, upper)\n",
      "/Users/jjhuang/MSD/CS6350/cs6350/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_zero_var_features = identify_zero_var_feat(train)\n",
    "\n",
    "# 載入測試資料\n",
    "test = pd.read_csv('data/test.csv')\n",
    "test = test.dropna()  # 確保賦值回test變數\n",
    "\n",
    "# 使用與訓練集相同的特徵集\n",
    "test_cleaned = test.copy()\n",
    "if set(train_features).issubset(set(test.columns)):\n",
    "    test_cleaned = test[['label'] + train_features]\n",
    "else:\n",
    "    # 如果測試集缺少特徵，添加這些特徵並設為0\n",
    "    for feat in train_features:\n",
    "        if feat not in test.columns:\n",
    "            test_cleaned[feat] = 0\n",
    "    test_cleaned = test_cleaned[['label'] + train_features]\n",
    "\n",
    "# 使用相同的前處理\n",
    "test_clean_and_trim = trim_outliers(test_cleaned)\n",
    "\n",
    "# 應用相同的轉換\n",
    "X_test = test_clean_and_trim.drop('label', axis=1)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test_pca = pca.transform(X_test_scaled)[:, :n_components]\n",
    "\n",
    "# 預測\n",
    "test_predictions = lr.predict(X_test_pca)\n",
    "\n",
    "# 計算準確率\n",
    "test_accuracy = accuracy_score(y_test_true, test_predictions)\n",
    "print(f\"測試資料準確率: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "c841d953-c31b-4987-acca-de72bef3b03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "準確率：0.8217\n",
      "\n",
      "分類報告：\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.95      0.77      0.85       993\n",
      "           1       0.68      0.92      0.78       527\n",
      "\n",
      "    accuracy                           0.82      1520\n",
      "   macro avg       0.81      0.84      0.82      1520\n",
      "weighted avg       0.85      0.82      0.83      1520\n",
      "\n",
      "\n",
      "特徵重要性（前10個）：\n",
      "   Feature  Coefficient\n",
      "68    PC69   -55.082167\n",
      "66    PC67    50.613907\n",
      "64    PC65   -49.469115\n",
      "12    PC13    45.322089\n",
      "56    PC57   -45.178237\n",
      "69    PC70   -41.651842\n",
      "76    PC77    40.065570\n",
      "18    PC19    37.362343\n",
      "55    PC56   -34.889029\n",
      "44    PC45    34.228908\n"
     ]
    }
   ],
   "source": [
    "# sns.scatterplot(data=train_p1, x='PC11', y='PC12', hue='label', palette={-1:'blue', 1:'red'})\n",
    "\n",
    "# 分割數據為特徵和標籤\n",
    "X = train_pca.drop('label', axis=1)\n",
    "y = train_pca['label'].map({-1: -1, 1: 1})  # 確保標籤是-1和1\n",
    "\n",
    "# 分割訓練集和測試集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 創建並訓練感知器模型\n",
    "perceptron = Perceptron(max_iter=1000, random_state=42)\n",
    "perceptron.fit(X_train, y_train)\n",
    "\n",
    "# 預測和評估\n",
    "# y_pred = perceptron.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Averaged Perceptron\n",
    "avg_perceptron = SGDClassifier(\n",
    "    loss=\"perceptron\",      # 感知器損失函數\n",
    "    average=True,           # 平均化權重（關鍵參數）\n",
    "    class_weight='balanced',\n",
    "    max_iter=400,\n",
    "    random_state=42,\n",
    "    alpha=0.00001,\n",
    "    eta0=0.7,\n",
    ")\n",
    "avg_perceptron.fit(X_train, y_train)\n",
    "y_pred_avg = avg_perceptron.predict(X_test)\n",
    "accuracy_avg = accuracy_score(y_test, y_pred_avg)\n",
    "report_avg = classification_report(y_test, y_pred_avg)\n",
    "\n",
    "print(f\"準確率：{accuracy_avg:.4f}\")\n",
    "print(\"\\n分類報告：\")\n",
    "print(report_avg)\n",
    "\n",
    "# 查看模型係數（重要性）\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': perceptron.coef_[0]\n",
    "}).sort_values('Coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(\"\\n特徵重要性（前10個）：\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "7cfbb4a2-c252-41dd-ac5b-e77d57a3813c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "交叉驗證準確率: 0.8144 (+/- 0.0223)\n",
      "最佳參數: {'alpha': 1e-05, 'eta0': 0.7, 'max_iter': 300}\n",
      "最佳交叉驗證準確率: 0.8155\n"
     ]
    }
   ],
   "source": [
    "# 5折交叉驗證\n",
    "cv_scores = cross_val_score(avg_perceptron, X, y, cv=5)\n",
    "print(f\"交叉驗證準確率: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")\n",
    "\n",
    "# 使用GridSearchCV同時進行交叉驗證和參數調優\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'alpha': [0.00001, 0.0001],\n",
    "    'max_iter': [300, 350, 400],\n",
    "    'eta0': [0.6, 0.7, 0.8]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    SGDClassifier(loss=\"perceptron\", average=True, class_weight='balanced'),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "grid.fit(X, y)\n",
    "print(f\"最佳參數: {grid.best_params_}\")\n",
    "print(f\"最佳交叉驗證準確率: {grid.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "f5d5e2bf-7f73-40b0-afc8-38efddac17a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "測試資料準確率: 0.8198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5c/g8dnzwpn14sbd87g_ts9nq980000gn/T/ipykernel_3727/2830798469.py:48: FutureWarning: Downcasting behavior in Series and DataFrame methods 'where', 'mask', and 'clip' is deprecated. In a future version this will not infer object dtypes or cast all-round floats to integers. Instead call result.infer_objects(copy=False) for object inference, or cast round floats explicitly. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data_trimmed[feat] = data_trimmed[feat].clip(lower, upper)\n",
      "/var/folders/5c/g8dnzwpn14sbd87g_ts9nq980000gn/T/ipykernel_3727/2830798469.py:48: FutureWarning: Downcasting behavior in Series and DataFrame methods 'where', 'mask', and 'clip' is deprecated. In a future version this will not infer object dtypes or cast all-round floats to integers. Instead call result.infer_objects(copy=False) for object inference, or cast round floats explicitly. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data_trimmed[feat] = data_trimmed[feat].clip(lower, upper)\n",
      "/var/folders/5c/g8dnzwpn14sbd87g_ts9nq980000gn/T/ipykernel_3727/2830798469.py:48: FutureWarning: Downcasting behavior in Series and DataFrame methods 'where', 'mask', and 'clip' is deprecated. In a future version this will not infer object dtypes or cast all-round floats to integers. Instead call result.infer_objects(copy=False) for object inference, or cast round floats explicitly. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data_trimmed[feat] = data_trimmed[feat].clip(lower, upper)\n",
      "/var/folders/5c/g8dnzwpn14sbd87g_ts9nq980000gn/T/ipykernel_3727/2830798469.py:48: FutureWarning: Downcasting behavior in Series and DataFrame methods 'where', 'mask', and 'clip' is deprecated. In a future version this will not infer object dtypes or cast all-round floats to integers. Instead call result.infer_objects(copy=False) for object inference, or cast round floats explicitly. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data_trimmed[feat] = data_trimmed[feat].clip(lower, upper)\n",
      "/var/folders/5c/g8dnzwpn14sbd87g_ts9nq980000gn/T/ipykernel_3727/2830798469.py:48: FutureWarning: Downcasting behavior in Series and DataFrame methods 'where', 'mask', and 'clip' is deprecated. In a future version this will not infer object dtypes or cast all-round floats to integers. Instead call result.infer_objects(copy=False) for object inference, or cast round floats explicitly. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data_trimmed[feat] = data_trimmed[feat].clip(lower, upper)\n",
      "/var/folders/5c/g8dnzwpn14sbd87g_ts9nq980000gn/T/ipykernel_3727/2830798469.py:48: FutureWarning: Downcasting behavior in Series and DataFrame methods 'where', 'mask', and 'clip' is deprecated. In a future version this will not infer object dtypes or cast all-round floats to integers. Instead call result.infer_objects(copy=False) for object inference, or cast round floats explicitly. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data_trimmed[feat] = data_trimmed[feat].clip(lower, upper)\n",
      "/Users/jjhuang/MSD/CS6350/cs6350/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but SGDClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_zero_var_features = identify_zero_var_feat(train)\n",
    "\n",
    "# 載入測試資料\n",
    "test = pd.read_csv('data/test.csv')\n",
    "test = test.dropna()  # 確保賦值回test變數\n",
    "\n",
    "# 使用與訓練集相同的特徵集\n",
    "test_cleaned = test.copy()\n",
    "if set(train_features).issubset(set(test.columns)):\n",
    "    test_cleaned = test[['label'] + train_features]\n",
    "else:\n",
    "    # 如果測試集缺少特徵，添加這些特徵並設為0\n",
    "    for feat in train_features:\n",
    "        if feat not in test.columns:\n",
    "            test_cleaned[feat] = 0\n",
    "    test_cleaned = test_cleaned[['label'] + train_features]\n",
    "\n",
    "# 使用相同的前處理\n",
    "test_clean_and_trim = trim_outliers(test_cleaned)\n",
    "\n",
    "# 應用相同的轉換\n",
    "X_test = test_clean_and_trim.drop('label', axis=1)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test_pca = pca.transform(X_test_scaled)[:, :n_components]\n",
    "\n",
    "# 將測試集標籤轉換為-1和1（與訓練集一致）\n",
    "y_test_true = test_clean_and_trim['label'].map({0: -1, 1: 1})\n",
    "\n",
    "# 預測\n",
    "test_predictions = avg_perceptron.predict(X_test_pca)\n",
    "\n",
    "# 計算準確率\n",
    "test_accuracy = accuracy_score(y_test_true, test_predictions)\n",
    "print(f\"測試資料準確率: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "7d32a7c0-8ee0-488a-82d6-b79eb3695846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "提交檔案已保存至 submission_logistic.csv\n",
      "預測標籤分佈:\n",
      "0 (非惡意): 1352\n",
      "1 (惡意): 1180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jjhuang/MSD/CS6350/cs6350/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but SGDClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def generate_submission(model, scaler, pca, n_components, output_file='submission_logistic.csv'):\n",
    "    \"\"\"\n",
    "    生成Kaggle提交檔案\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : 訓練好的模型\n",
    "    scaler : 已經擬合的StandardScaler\n",
    "    pca : 已經擬合的PCA\n",
    "    n_components : PCA保留的組件數量\n",
    "    output_file : 輸出檔案名稱，預設為'submission.csv'\n",
    "    \"\"\"\n",
    "    # 讀取評估資料\n",
    "    eval_data = pd.read_csv('data/eval.anon.csv')\n",
    "    # 讀取ID對照表\n",
    "    eval_ids = pd.read_csv('data/eval.ids', header=None)\n",
    "    eval_ids.columns = ['example_id']\n",
    "    \n",
    "    # 提取特徵\n",
    "    X_eval = eval_data.drop('label', axis=1)\n",
    "    \n",
    "    # 零方差特徵處理 (使用與訓練集相同的處理方式)\n",
    "    train_zero_var_features = identify_zero_var_feat(train)\n",
    "    train_features = [col for col in train.columns if col != 'label' and col not in train_zero_var_features]\n",
    "    \n",
    "    # 確保評估集有相同的特徵\n",
    "    for feat in train_features:\n",
    "        if feat not in X_eval.columns:\n",
    "            X_eval[feat] = 0\n",
    "    X_eval = X_eval[train_features]\n",
    "    \n",
    "    # 應用相同的預處理流程\n",
    "    X_eval = trim_outliers(X_eval)\n",
    "    \n",
    "    # 標準化和PCA轉換\n",
    "    X_eval_scaled = scaler.transform(X_eval)\n",
    "    X_eval_pca = pca.transform(X_eval_scaled)[:, :n_components]\n",
    "    \n",
    "    # 預測\n",
    "    predictions = model.predict(X_eval_pca)\n",
    "    \n",
    "    # 將-1轉換回0 (如果模型輸出是-1和1)\n",
    "    predictions = np.where(predictions == -1, 0, predictions)\n",
    "    \n",
    "    # 創建提交檔案\n",
    "    submission = pd.DataFrame({\n",
    "        'example_id': eval_ids['example_id'],\n",
    "        'label': predictions\n",
    "    })\n",
    "    \n",
    "    # 保存為CSV\n",
    "    submission.to_csv(output_file, index=False)\n",
    "    print(f\"提交檔案已保存至 {output_file}\")\n",
    "    \n",
    "    # 檢查標籤分佈\n",
    "    label_counts = pd.Series(predictions).value_counts()\n",
    "    print(\"預測標籤分佈:\")\n",
    "    print(f\"0 (非惡意): {label_counts.get(0, 0)}\")\n",
    "    print(f\"1 (惡意): {label_counts.get(1, 0)}\")\n",
    "    \n",
    "    return submission\n",
    "\n",
    "# submission = generate_submission(\n",
    "#     model=lr,  # 您的模型\n",
    "#     scaler=scaler,         # 已擬合的標準化器\n",
    "#     pca=pca,               # 已擬合的PCA\n",
    "#     n_components=n_components  # PCA組件數量\n",
    "# )\n",
    "    \n",
    "submission = generate_submission(\n",
    "    model=avg_perceptron,  # 您的模型\n",
    "    scaler=scaler,         # 已擬合的標準化器\n",
    "    pca=pca,               # 已擬合的PCA\n",
    "    n_components=n_components  # PCA組件數量\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a685a794-ec17-4196-ad20-854ebbda49e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
